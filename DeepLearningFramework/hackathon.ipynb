{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "# from keras.datasets import patient\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from io import StringIO\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     
    }
   ],
   "source": [
    "pandas.set_option('display.max_colwidth', 1000)\n",
    "print(os.getcwd())\n",
    "# dataset = pandas.read_csv(StringIO(s), quotechar='\"', skipinitialspace=True, error_bad_lines = False)\n",
    "dataset = pandas.read_csv('record.csv')\n",
    "dataset = pandas.DataFrame(dataset)\n",
    "dataset = dataset.dropna(axis=0, how='any')\n",
    "X = dataset['symptoms']\n",
    "Y = dataset['disease']\n",
    "\n",
    "# encode class values as integers\n",
    "encoder_Y = LabelEncoder()\n",
    "docs2 = [\"arthritis\",\"asthma\",\"ASD\",\"cancer\",\"chlamydia\",\"ebola\",\"diabetes\",\"malaria\",\"HIV\",\"cirrhosis\",\"migraine\", \n",
    "        \"heart disease\",\"rhinovirus\",\"influenza\",\"stroke\",\"alzheimer disease\",\"tuberculosis\",\"thyroid\"]\n",
    "encoder_Y.fit(docs2)\n",
    "encoded_Y = encoder_Y.transform(Y)\n",
    "print(encoded_Y[2])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y = np_utils.to_categorical(encoded_Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    
   "source": [
    "\n",
    "# define 5 documents\n",
    "docs = [\"headache\",\"nausea\",\"vomiting\",\"high fever\",\"diarrhea\",\"fatigue\",\"muscle aches\",\"coughing\",\"pain\",\"stiffness\",\n",
    "        \"swelling\",\"redness\",\"wheezing\",\"learning disability\",\"epilepsy\",\"chronic pain\",\"blood discharge\",\"sneezing\",\n",
    "        \"pain during urination\",\"genital pain\",\"palpitation\",\"weakness\",\"faster heartbeat\", \"nasal dryness\",\"sore throat\",\n",
    "        \"abdominal pain\",\"weight loss\",\"chills\",\"chest pain\",\"back pain\",\"tired\",\"blindness\",\"blurred vision\",\"dry mouth\",\n",
    "        \"paralysis\",\"dizziness\",\"memory loss\",\"nose bleed\",\"jaundice\",\"constipation\"]\n",
    "\n",
    "vocab_size = 70\n",
    "max_review_length = 200\n",
    "MAX_NUM_WORDS = 1000\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "X_train_vector = vectorizer.transform(X_train)\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "X_train =  X_train_vector.toarray()\n",
    "X_test =  X_test_vector.toarray()\n",
    "#padding the data\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length, dtype='object', \n",
    "                                           padding='pre', truncating='pre', value=0.)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length, dtype='object', \n",
    "                                           padding='pre', truncating='pre', value=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 200, 32)           1280      \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 18)                1818      \n",
      "=================================================================\n",
      "Total params: 56,298\n",
      "Trainable params: 56,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 79999 samples, validate on 20000 samples\n",
      "Epoch 1/3\n",
      "79999/79999 [==============================] - 205s 3ms/step - loss: 0.7671 - acc: 0.7807 - val_loss: 0.4270 - val_acc: 0.8571\n",
      "Epoch 2/3\n",
      "79999/79999 [==============================] - 202s 3ms/step - loss: 0.4735 - acc: 0.8462 - val_loss: 0.4210 - val_acc: 0.8571\n",
      "Epoch 3/3\n",
      "79999/79999 [==============================] - 202s 3ms/step - loss: 0.4210 - acc: 0.8570 - val_loss: 0.4218 - val_acc: 0.8571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1113e089b00>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(200, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(18, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.72%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.71%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"D:\\\\Hackathon\\\\trained_model\\\\model_categorical.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"D:\\\\Hackathon\\\\trained_model\\\\model_categorical.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    " \n",
    "# # evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
  
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
